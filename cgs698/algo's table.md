## üß† Summary of Bayesian Sampling Algorithms

| **Algorithm**              | **Description**                                             | **Key Steps**                                                                                                                                               | **Pros**                                                  | **Cons**                                                                 |
|----------------------------|-------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------|-------------------------------------------------------------------------|
| **Grid Approximation**     | Discretize parameter space and compute exact posterior      | 1. Define a grid of Œ∏ values  <br> 2. Compute likelihood √ó prior at each grid point  <br> 3. Normalize to get posterior                                      | ‚úÖ Simple to implement  <br> ‚úÖ Exact for 1D problems       | ‚ùå Slow in high dimensions  <br> ‚ùå Grid resolution affects accuracy     |
| **Monte Carlo Integration**| Estimate integrals (e.g., marginal likelihood) by averaging | 1. Draw Œ∏·µ¢ from prior  <br> 2. Compute p(y|Œ∏·µ¢)  <br> 3. Average those likelihoods to approximate the integral                                                | ‚úÖ Easy to code  <br> ‚úÖ No rejection step                  | ‚ùå Doesn‚Äôt give posterior samples  <br> ‚ùå Inefficient if prior ‚â† posterior |
| **Rejection Sampling**     | Accept/reject samples from proposal based on posterior shape| 1. Propose Œ∏* from g(Œ∏)  <br> 2. Compute score = [p(y|Œ∏*)p(Œ∏*)] / [M g(Œ∏*)]  <br> 3. Accept if u < score (u ~ Uniform[0,1])                                   | ‚úÖ Independent posterior samples  <br> ‚úÖ Conceptually clear | ‚ùå Many samples may be rejected  <br> ‚ùå Need to tune M                   |
| **Metropolis‚ÄìHastings**    | MCMC using random-walk proposals and acceptance ratio       | 1. Start at Œ∏‚ÇÄ  <br> 2. Propose Œ∏* ~ q(¬∑|Œ∏·µ¢)  <br> 3. Compute H = ratio of posteriors  <br> 4. Accept Œ∏* with probability min(1, H)                          | ‚úÖ Works for complex posteriors  <br> ‚úÖ No need for normalizing constant | ‚ùå Samples are correlated  <br> ‚ùå Requires tuning of proposal scale     |
| **Gibbs Sampling**         | MCMC by cycling through conditional distributions           | 1. Initialize all parameters  <br> 2. Sample each Œ∏‚±º from p(Œ∏‚±º | rest) one at a time  <br> 3. Repeat N times                                                  | ‚úÖ No rejection step  <br> ‚úÖ Often fast when conditionals are known     | ‚ùå Needs full conditional distributions  <br> ‚ùå Can be slow to mix       |
                   |                                                             |

---

## üîÅ Summary of Use-Cases

| **If you want to...**                                        | **Use this algorithm**     |
| ------------------------------------------------------------ | -------------------------- |
| Estimate marginal likelihood or expectation                  | Monte Carlo Integration    |
| Get independent posterior samples (low dim)                  | Rejection Sampling         |
| Sample from complex posterior (general-purpose)              | Metropolis‚ÄìHastings (MCMC) |
| Sample from posterior with easy conditionals (multi-variate) | Gibbs Sampling             |

---
