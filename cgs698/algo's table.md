## üìä Table: Comparison of Bayesian Sampling Algorithms

| **Algorithm**                  | **Description**                                                                                                                      | **Pros ‚úÖ**                                                        | **Cons ‚ùå**                                                                      | **Key Steps**                                                       |                                                                                        |                                                     |
| ------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | --------------------------------------------------- |
| **Monte Carlo Integration**    | Approximates integrals (e.g., marginal likelihood) by averaging likelihoods over samples drawn from the **prior**.                   | Simple to implement, useful for evidence estimation               | Doesn‚Äôt generate posterior samples, inefficient if prior ‚â† posterior            | 1. Sample Œ∏ from prior<br>2. Compute likelihood p(y                 | Œ∏)<br>3. Take average of all p(y                                                       | Œ∏)                                                  |
| **Rejection Sampling**         | Samples from the **posterior** by accepting/rejecting samples from a proposal distribution based on how well they fit the posterior. | Easy conceptually, gives independent samples                      | High rejection rate, inefficient in high dimensions                             | 1. Sample Œ∏ from proposal g(Œ∏)<br>2. Compute acceptance score = p(y | Œ∏)p(Œ∏)/\[M¬∑g(Œ∏)]<br>3. Accept Œ∏ if u < score                                           |                                                     |
| **Metropolis‚ÄìHastings (MCMC)** | Samples from posterior using a **Markov chain**. Proposes new Œ∏, accepts it probabilistically based on posterior ratio.              | Works for complex posteriors, no need for normalized densities    | Requires tuning proposal, correlated samples, slow convergence in some cases    | 1. Initialize Œ∏‚ÇÄ<br>2. Propose Œ∏\* from q(Œ∏\*                       | Œ∏·µ¢)<br>3. Compute acceptance ratio H<br>4. Accept Œ∏\* with prob min(1, H)<br>5. Repeat |                                                     |
| **Gibbs Sampling (MCMC)**      | Special case of MCMC where **each parameter** is sampled from its **full conditional distribution**, holding others fixed.           | No rejections, easy for high dimensions if conditionals are known | Requires closed-form full conditionals, slow mixing if variables are correlated | 1. Initialize all Œ∏s<br>2. Loop: Sample Œ∏‚ÇÅ                          | Œ∏‚ÇÇ, ..., Œ∏‚Çô; then Œ∏‚ÇÇ                                                                   | Œ∏‚ÇÅ, Œ∏‚ÇÉ, ..., Œ∏‚Çô; etc.<br>3. Repeat for N iterations |

---

## üîÅ Summary of Use-Cases

| **If you want to...**                                        | **Use this algorithm**     |
| ------------------------------------------------------------ | -------------------------- |
| Estimate marginal likelihood or expectation                  | Monte Carlo Integration    |
| Get independent posterior samples (low dim)                  | Rejection Sampling         |
| Sample from complex posterior (general-purpose)              | Metropolis‚ÄìHastings (MCMC) |
| Sample from posterior with easy conditionals (multi-variate) | Gibbs Sampling             |

---
