
### ðŸ“˜ Topic 4: **Grid Approximation**

---

### ðŸ§  Whatâ€™s the idea?

Letâ€™s say you're playing **"Guess the number"**, and you know itâ€™s between 1 and 10.

One way to solve it is to just **try every number**:

* Try 1. Too low?
* Try 2. Still low?
* ...
* Try 7. Just right!

ðŸŽ¯ Thatâ€™s the idea of **Grid Approximation**.
You divide the range of possible Î¸ values into **equally spaced points** (called a **grid**), then:

1. Compute how **likely** each point is, using your data and prior.
2. Use this info to **approximate** the posterior.

---

### ðŸªœ Step-by-step Method

Letâ€™s say Î¸ can be from 0 to 1. You:

1. Create a grid like:
   `Î¸ = [0.0, 0.1, 0.2, ..., 1.0]`
2. For each Î¸:

   * Compute **likelihood** = how likely this Î¸ explains the data
   * Compute **prior** = how much you believed in this Î¸ before
3. Multiply them together:
   `posterior_Î¸ = likelihood Ã— prior`
4. Normalize (make it sum to 1) â†’ This gives you an **approximate posterior**!

---

## ðŸ§ª Example: **Normal Model with Unknown Mean (Âµ), Known Variance (ÏƒÂ² = 4)**

---

### ðŸ‘¶ Imagine This Setup:

* You **measured something 10 times** (like how long people take to read a sentence).
* You **believe** these reading times come from a **normal distribution** with:

  * Unknown mean = Î¼ (what you're trying to find!)
  * Known variance = ÏƒÂ² = 4 â†’ so standard deviation Ïƒ = 2
* You also **believe before starting** (prior) that the mean Î¼ is probably near 0, but could vary a lot:

  * So you say Î¼ \~ Normal(0, 9) (Ïƒ\_priorÂ² = 9 â†’ standard deviation = 3)

---

### ðŸ“Š Step 1: **Observed Data**

The data (10 reading times) is generated like this in R:

```r
y <- rnorm(10, mean=1, sd=2)
```

This simulates 10 values from Normal(1, 2), for example:

```
[1] 2.57 3.06 1.86 3.59 -0.25 1.48 -0.15 1.64 0.70 2.44
```

These are your 10 measurements (`y`).

---

### ðŸ§® Step 2: **Analytical Posterior (Ground Truth)**

If you use the conjugate math shortcut, you could compute the exact posterior of Î¼ as another normal distribution.

This part is calculated as:

```r
mean_post <- ((ÏƒÂ² Ã— Î¼â‚€) + (Ïƒâ‚€Â² Ã— sum(y))) / (ÏƒÂ² + n Ã— Ïƒâ‚€Â²)
sd_postÂ² <- 1 / ((1/Ïƒâ‚€Â²) + (n/ÏƒÂ²))
```

You draw samples from this normal distribution and get a **histogram** showing where the true mean likely lies.

ðŸ“Š This gives a nice **reference solution** to compare against grid approximation.

---

### ðŸ§® Step 3: **Grid Approximation**

Now we simulate the same thing **without the math shortcut**:

#### âœ… 1. Make a grid of Âµ values:

```r
mu_grid <- seq(-10, 10, length=1000)
```

Thatâ€™s 1000 evenly spaced guesses between -10 and 10.

#### âœ… 2. For each guess:

We calculate:

* **Likelihood**: how likely is the data if this guess were true?

  ```r
  likelihood <- prod(dnorm(y, mu_grid[i], 2))
  ```

  Thatâ€™s the product of 10 probabilities â€” one for each data point.

* **Prior**: how believable is this guess before seeing data?

  ```r
  prior <- dnorm(mu_grid[i], 0, 3)
  ```

* **Posterior (unnormalized)**:

  ```r
  posterior = likelihood Ã— prior
  ```

We store all these values in a table (`df.posterior`).

#### âœ… 3. Normalize the posterior:

We divide each posterior value by the total sum of all posterior values across the grid:

```r
posterior[i] = (likelihood Ã— prior) / sum(likelihood Ã— prior for all grid points)
```

Now all the posterior values **add up to 1** â€” a proper probability distribution!

---

### ðŸ–¼ï¸ Step 4: **Plot the posterior**

We now plot `posterior vs mu`.

```r
plot(df.posterior$mu, df.posterior$posterior)
```

This gives a **curve** that looks like a mountainâ€”centered around the most likely value of Âµ.

âœ… The peak of this curve should be very close to the mean you got using the exact formula (from the analytical method earlier).

---

### ðŸ§¸ Final Analogy:

> Imagine youâ€™re guessing a personâ€™s height based on noisy measurements.
> You try every possible height between 4ft and 8ft (grid).
> For each guess, you check:
>
> * How well it fits your data (likelihood)?
> * How much you trusted that height before (prior)?
>   You combine the two to figure out how much to believe in each guess now (posterior).

---

**TL;DR of the Example:**

> You guessed many possible values of Âµ, checked how well each explains the data and fits your prior belief, and used those to draw a â€œmountain-shapedâ€ posterior curve showing where Âµ probably lies.

---

### ðŸ§¸ Analogy:

Think of it like a **heat map** in a video game:

* You scan the whole field, square by square.
* Areas with lots of treasure (good Î¸ values) glow **hot**.
* The brighter it glows, the more you believe thatâ€™s where the treasure (true Î¸) is!

---

### âš ï¸ Drawbacks (What can go wrong):

* ðŸ”º **Too many parameters?** Trouble!

  * If Î¸ has many parts (like 5 or 10 parameters), the number of grid points explodes:

    * For 4 parameters with 1000 points each: $1000^4 = 1,000,000,000,000$ grid points ðŸ¤¯
* ðŸ”» **Reduce points?** You lose accuracy.

This is called the **â€œcurse of dimensionality.â€**

---

**TL;DR in Kidspeak:**

> Grid approximation means guessing values on a grid, checking how good each is, and combining that to see where the best guesses are. But it works best when thereâ€™s only 1 or 2 things to guess.

---

### **R code** for **Grid Approximation**

```r
# -------------------------------
# Grid Approximation in R
# -------------------------------

# STEP 1: Simulate observed data (10 samples from Normal with mean = 1, sd = 2)
set.seed(42)  # for reproducibility
y <- rnorm(10, mean = 1, sd = 2)  # observed data
n <- length(y)  # number of observations

# STEP 2: Set prior belief for the mean (mu)
mu_prior_mean <- 0      # prior mean
mu_prior_sd <- 3        # prior standard deviation

# STEP 3: Create a grid of mu values to evaluate
mu_grid <- seq(-10, 10, length.out = 1000)  # 1000 equally spaced values from -10 to 10

# STEP 4: Initialize a data frame to store results
df.posterior <- data.frame(mu = mu_grid,
                           likelihood = NA,
                           prior = NA,
                           posterior = NA)

# STEP 5: Loop through each grid point and compute likelihood and prior
for(i in seq_along(mu_grid)) {
  mu_i <- mu_grid[i]

  # Likelihood: product of normal PDFs for each observed y given current mu and known sd = 2
  likelihood <- prod(dnorm(y, mean = mu_i, sd = 2))

  # Prior: normal PDF of current mu_i given prior mean and SD
  prior <- dnorm(mu_i, mean = mu_prior_mean, sd = mu_prior_sd)

  # Save values
  df.posterior$likelihood[i] <- likelihood
  df.posterior$prior[i] <- prior
}

# STEP 6: Compute unnormalized posterior (likelihood Ã— prior)
df.posterior$unnorm_posterior <- df.posterior$likelihood * df.posterior$prior

# STEP 7: Normalize to get proper posterior probabilities (so they sum to 1)
marginal_likelihood <- sum(df.posterior$unnorm_posterior)
df.posterior$posterior <- df.posterior$unnorm_posterior / marginal_likelihood

# STEP 8: Plot the approximate posterior distribution
plot(df.posterior$mu, df.posterior$posterior, type = "l", col = "blue", lwd = 2,
     main = "Grid Approximation of Posterior",
     xlab = "mu", ylab = "Posterior Probability Density")

# Optional: Add vertical line at true mean used to simulate data
abline(v = 1, col = "red", lty = 2)
```

---
