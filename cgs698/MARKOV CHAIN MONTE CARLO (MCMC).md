## ğŸ“˜ MARKOV CHAIN MONTE CARLO (MCMC)

---

### ğŸ§  DEFINITION

> **MCMC** is a family of algorithms to draw samples from a complicated distribution (like the posterior) by constructing a **Markov chain** whose equilibrium distribution is the target. Over many steps, the chain â€œforgetsâ€ its start and produces samples from the desired distribution.

---

### ğŸ› ï¸ METHODOLOGY (High-Level)

1. **Initialize** the chain at some starting value Î¸â‚€.
2. **Propose** a new value Î¸\* based on the current Î¸áµ¢, using a **proposal distribution** q(Î¸\* | Î¸áµ¢).
3. **Compute** the **acceptance ratio**

   $$
     H = \frac{p(y|\theta^*)\,p(\theta^*)\,q(\theta_i|\theta^*)}{p(y|\theta_i)\,p(\theta_i)\,q(\theta^*|\theta_i)}
   $$
4. **Accept** Î¸\* with probability Î± = min(1, H).
   Otherwise **keep** Î¸áµ¢.
5. **Iterate** steps 2â€“4 to build a long chain Î¸â‚, Î¸â‚‚, â€¦, Î¸\_N.
6. After â€œburn-inâ€ (initial steps), the samples approximate the target posterior.

---

### ğŸ§¸ Child-Friendly Analogy

> Imagine a puppy sniffing for buried treats in a yard.
>
> * It starts at one spot.
> * At each step, it **sniffs around** nearby (proposal).
> * If the smell is stronger (higher posterior density), it **moves** there.
> * If weaker, it sometimes still moves, but often stays.
>   Over time, the puppy spends more time in the tastiest spotsâ€”just like MCMC spends more samples in high-probability regions.

---

## ğŸ”„ STEP-BY-STEP ALGORITHM (Metropolisâ€“Hastings)

1. **Choose**:

   * A starting value Î¸â‚€.
   * A proposal distribution q(Â·|Â·), e.g., Normal centered at current Î¸.
   * Number of samples N and burn-in B.
2. **For** i = 0 to Nâ€“1:

   1. **Sample** Î¸\* âˆ¼ q(Â· | Î¸áµ¢).
   2. **Compute**

      $$
      H = \frac{p(y|\theta^*)\,p(\theta^*)\,q(\theta_i|\theta^*)}{p(y|\theta_i)\,p(\theta_i)\,q(\theta^*|\theta_i)}.
      $$
   3. **Set**

      $$
        Î¸_{i+1} = 
        \begin{cases}
          Î¸^*, & \text{with probability } \min(1, H),\\
          Î¸_i, & \text{otherwise.}
        \end{cases}
      $$
3. **Discard** the first B samples, and use the rest for inference.

---

## ğŸ§ª EXAMPLE: Beta-Binomial via MCMC

* Data: k = 2 heads in n = 10 flips
* Prior: Î¸ âˆ¼ Beta(1,1)
* Likelihood: Binomial(n=10, Î¸)

---

### ğŸ’» R CODE WITH EXPLANATIONS

```r
# 1. Set up
k <- 2
n <- 10
a <- 1; b <- 1         # Beta(1,1) prior

N <- 20000             # total MCMC iterations
burn_in <- 5000        # burn-in
theta_chain <- numeric(N)
theta_chain[1] <- 0.5  # starting value

proposal_sd <- 0.1     # step-size for Normal proposal

# 2. MCMC loop
for (i in 1:(N-1)) {
  current <- theta_chain[i]
  
  # 2.1 Propose a new value
  theta_star <- rnorm(1, mean=current, sd=proposal_sd)
  
  # 2.2 If outside [0,1], reject immediately
  if (theta_star <= 0 || theta_star >= 1) {
    theta_chain[i+1] <- current
    next
  }
  
  # 2.3 Compute log posterior (for stability)
  log_post <- function(theta) {
    dbeta(theta, a, b, log=TRUE) + 
      dbinom(k, n, theta, log=TRUE)
  }
  
  log_H <- log_post(theta_star) - log_post(current)
  
  # 2.4 Accept/reject
  if (log(runif(1)) < log_H) {
    theta_chain[i+1] <- theta_star
  } else {
    theta_chain[i+1] <- current
  }
}

# 3. Discard burn-in and inspect
posterior_samples <- theta_chain[(burn_in+1):N]

# 4. Plot estimated posterior
hist(posterior_samples, freq=FALSE, breaks=30,
     main="MCMC Posterior", xlab="theta")
```

---

### ğŸ§  Explanation of Key Steps

* **Proposal**: `rnorm(current, sd=proposal_sd)`
  â†’ small random â€œjumpsâ€ around the current Î¸.
* **Log posterior**: we combine prior and likelihood in log-space to avoid underflow.
* **Acceptance**:
  $\log U < \log p^* - \log p$ is equivalent to $U < H$.

---

## ğŸ¯ Choosing Parameters

* **proposal\_sd**:

  * Too small â†’ chain moves slowly (high autocorrelation).
  * Too large â†’ most proposals are rejected.
  * Aim for \~25â€“50% acceptance rate.
* **burn\_in**:

  * Discard initial â€œwarm-upâ€ period before the chain reaches equilibrium.

---

## âœ… Benefits & Drawbacks

| Benefit                                       | Drawback                                       |
| --------------------------------------------- | ---------------------------------------------- |
| Samples directly from the posterior           | Requires tuning (step-size, burn-in)           |
| Works for high dimensions if tuned well       | Samples are correlated; need enough iterations |
| No need for normalizing constant of posterior | Diagnostics (trace plots, autocorrelation)     |

---

### ğŸ§¸ Puppy Analogy Revisited

> The puppyâ€™s **step size** is `proposal_sd`. If too small, it wanders slowly; if too big, it often jumps out of the yard and has to go back (rejected).

---

**TL;DR:**

> MCMC builds a â€œrandom walkâ€ that spends more time in high-probability regions. Over many steps, the samples approximate the true posterior.

---

Perfect! Let's now write a **clear pseudo-code** for the **Metropolisâ€“Hastings (MCMC)** algorithm, followed by a **dry run** with simple made-up numbers to show how each step works.

---

## ğŸ“˜ Metropolisâ€“Hastings: Pseudo-Code

---

```text
Input:
- p(y | Î¸): Likelihood function
- p(Î¸): Prior distribution
- q(Î¸* | Î¸): Proposal distribution (symmetric, e.g., Normal)
- Î¸â‚€: Initial value
- N: Total number of samples
- Ïƒ: Proposal standard deviation

Output:
- Î¸â‚, Î¸â‚‚, ..., Î¸_N: Samples from the posterior

Algorithm:
1. Initialize Î¸â‚€
2. For i = 1 to N:
   a. Propose Î¸* ~ q(Î¸* | Î¸_{i-1})  â† sample from proposal
   b. Compute acceptance ratio H:
        H = [p(y | Î¸*) * p(Î¸*)] / [p(y | Î¸_{i-1}) * p(Î¸_{i-1})]
   c. Draw u ~ Uniform(0,1)
   d. If u < H:
        Î¸áµ¢ â† Î¸*
      Else:
        Î¸áµ¢ â† Î¸_{i-1}
3. Return Î¸â‚ to Î¸_N
```

If the proposal distribution is **symmetric** (e.g., Normal), the q terms cancel out.

---

## ğŸ§ª DRY RUN: With Simple Numbers

Letâ€™s make it super easy and use tiny integers instead of actual binomial/Beta math.

---

### ğŸ¯ Setup:

We pretend:

* Î¸ âˆˆ \[0, 1], but use discrete values {0.1, 0.2, ..., 0.9}
* Likelihood (made up):
  $p(y|\theta) = \theta^2$
* Prior (uniform): $p(\theta) = 1$
* Proposal: Gaussian jump of Â±0.1
* Start at Î¸â‚€ = 0.4
* Do 5 steps

---

### ğŸ§¾ Simulated Values

We'll use simple values for calculations.

\| Î¸   | Likelihood $p(y|\theta)$ | Prior | Posterior Score $\propto \text{likelihood Ã— prior}$ |
\|------|-------------------------------|--------|---------------------------------------------------------|
\| 0.1  | 0.01                          | 1      | 0.01                                                    |
\| 0.2  | 0.04                          | 1      | 0.04                                                    |
\| 0.3  | 0.09                          | 1      | 0.09                                                    |
\| 0.4  | 0.16                          | 1      | 0.16                                                    |
\| 0.5  | 0.25                          | 1      | 0.25                                                    |
\| 0.6  | 0.36                          | 1      | 0.36                                                    |
\| 0.7  | 0.49                          | 1      | 0.49                                                    |
\| 0.8  | 0.64                          | 1      | 0.64                                                    |
\| 0.9  | 0.81                          | 1      | 0.81                                                    |

---

### ğŸ” Step-by-Step Simulation

#### Step 0:

* Start at Î¸â‚€ = 0.4
* Score = 0.16

---

#### Step 1:

* Propose Î¸\* = 0.5
* Score = 0.25
* Acceptance Ratio:

  $$
  H = \frac{0.25}{0.16} = 1.5625 > 1 â‡’ \text{accept}
  $$
* Move to Î¸â‚ = 0.5

---

#### Step 2:

* Propose Î¸\* = 0.6
* Score = 0.36
* H = 0.36 / 0.25 = 1.44 â†’ Accept
* Î¸â‚‚ = 0.6

---

#### Step 3:

* Propose Î¸\* = 0.5 (backward move)
* Score = 0.25
* H = 0.25 / 0.36 = 0.694
* Draw u = 0.4 â†’ u < H â†’ Accept
* Î¸â‚ƒ = 0.5

---

#### Step 4:

* Propose Î¸\* = 0.4
* Score = 0.16
* H = 0.16 / 0.25 = 0.64
* Draw u = 0.75 â†’ u > H â†’ Reject
* Î¸â‚„ = Î¸â‚ƒ = 0.5

---

#### Step 5:

* Propose Î¸\* = 0.6
* Score = 0.36
* H = 0.36 / 0.25 = 1.44 â†’ Accept
* Î¸â‚… = 0.6

---

### ğŸ§¾ Final Chain:

```
Î¸â‚€ = 0.4
Î¸â‚ = 0.5
Î¸â‚‚ = 0.6
Î¸â‚ƒ = 0.5
Î¸â‚„ = 0.5 (rejected move)
Î¸â‚… = 0.6
```

âœ… The chain **moves more often toward higher posterior** values
âœ… Sometimes accepts lower ones (adds randomness)

---

### ğŸ“Š After Many Steps...

If we repeat this process thousands of times, the **histogram of Î¸ values** will match the shape of the true posterior.
